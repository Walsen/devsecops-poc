"""Interactive Penetration Testing Agent using Strands Agents."""
import asyncio
import os
import subprocess
from datetime import datetime, timedelta, timezone
from functools import lru_cache
from pathlib import Path
from typing import Any

import boto3
from strands import Agent, tool
from strands.models.bedrock import BedrockModel


class PentestAgent:
    """Agent for interactive penetration testing with auto-debugging."""
    
    def __init__(self):
        self.cf_client = boto3.client("cloudformation", region_name="us-east-1")
        self.logs_client = boto3.client("logs", region_name="us-east-1")
        self.wafv2_client = boto3.client("wafv2", region_name="us-east-1")
        
        # Get testing directory (where this script is located)
        self.testing_dir = Path(__file__).parent.resolve()
        
        # Cache for test results and AWS resources
        self._test_cache = {}
        self._cache_ttl = 300  # 5 minutes
        
        # Track test history for smart selection
        self._test_history = {
            "failures": [],
            "slow_tests": ["test-nmap", "test-nikto", "test-sqlmap"],
        }
        
        # Verbose mode â€” stream tool output to terminal
        self._verbose = os.environ.get("PENTEST_VERBOSE", "1") == "1"
        
        # Model selection â€” default to Haiku 4.5 (fast + cheap for tool-heavy workflows)
        # Newer models require inference profiles (us. prefix) instead of raw model IDs
        model_id = os.environ.get(
            "PENTEST_MODEL_ID",
            "us.anthropic.claude-haiku-4-5-20251001-v1:0",
        )
        model = BedrockModel(model_id=model_id, region_name="us-east-1")
        
        self.agent = Agent(
            name="PentestAgent",
            model=model,
            system_prompt="""You are a penetration testing assistant. Your ONLY purpose is to help
run, debug, and fix the security tests in this project's testing suite.

## GUARDRAILS â€” STRICTLY ENFORCED

You MUST refuse any request that falls outside these boundaries:
1. You may ONLY run tests defined in the testing/ directory via the provided tools.
2. You may ONLY read and modify files inside the testing/ directory.
3. You may ONLY query AWS resources for debugging test failures (CloudWatch Logs,
   CloudFormation outputs, WAF configs). You must NOT create, modify, or delete
   any AWS resources.
4. You must NOT execute arbitrary shell commands. All execution goes through the
   provided test runner tools (run_pytest_test, run_all_tests_parallel, run_fast_tests).
5. You must NOT help with tasks unrelated to penetration testing, such as writing
   application code, deploying infrastructure, managing databases, or general coding.
6. If a user asks you to do something outside these boundaries, politely decline
   and explain that you can only help with running and debugging security tests.

## WORKFLOW

1. Ask the user which test they want to run (or suggest based on history).
2. Execute tests using the provided tools.
3. ALWAYS show the test execution output so the user can see results.
4. If a test fails, analyze the error.
5. Use AWS read-only tools to find relevant resources (log groups, WAF ARNs, etc.).
6. Fix test code using fix_test_code (testing/ files only).
7. Re-run the test to confirm the fix.
8. Explain what you found and fixed.

## AVAILABLE TESTS

- test-health: Health check endpoint (fast)
- test-headers: Security headers validation (fast)
- test-sql-injection: SQL injection protection (fast)
- test-xss: XSS sanitization (fast)
- test-rate-limit: Rate limiting enforcement (medium)
- test-idor: IDOR protection (creates Cognito test users, needs auth)
- test-nmap: Port scanning (slow â€” skip unless requested)
- test-nikto: Web vulnerability scan (slow â€” skip unless requested)
- test-sqlmap: Automated SQL injection scan (slow â€” skip unless requested)

## OPTIMIZATION TIPS

- Use run_all_tests_parallel for multiple fast tests.
- Skip slow tests unless explicitly requested.
- Use cached AWS resources when available.
- Suggest running fast tests first for quick feedback.

Be conversational and explain what you're doing at each step.""",
            tools=[
                self.run_pytest_test,
                self.run_all_tests_parallel,
                self.run_fast_tests,
                self.get_cloudformation_outputs,
                self.list_log_groups,
                self.query_logs,
                self.list_waf_webacls,
                self.get_waf_logging_config,
                self.read_test_file,
                self.fix_test_code,
                self.get_test_recommendations,
            ],
        )
    
    # Allowed test names (prevent arbitrary command execution)
    ALLOWED_TESTS = frozenset({
        "test", "test-parallel", "test-fast", "test-verbose", "test-report",
        "test-health", "test-headers", "test-sql-injection", "test-xss",
        "test-rate-limit", "test-nmap", "test-nikto", "test-sqlmap", "test-idor",
    })

    def _run_command(self, cmd: list[str], env: dict | None = None) -> tuple[int, str, str]:
        """Run a command, streaming output to terminal in verbose mode.
        
        Returns:
            Tuple of (return_code, stdout, stderr)
        """
        run_env = env or os.environ.copy()

        if self._verbose:
            # Stream output to terminal in real-time while capturing it
            proc = subprocess.Popen(
                cmd,
                cwd=self.testing_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                env=run_env,
            )
            output_lines = []
            for line in proc.stdout:
                print(line, end="", flush=True)
                output_lines.append(line)
            proc.wait()
            combined = "".join(output_lines)
            return proc.returncode, combined, ""
        else:
            result = subprocess.run(
                cmd,
                cwd=self.testing_dir,
                capture_output=True,
                text=True,
                env=run_env,
            )
            return result.returncode, result.stdout, result.stderr

    @tool
    def run_pytest_test(self, test_name: str, skip_cloudwatch: bool = False) -> dict[str, Any]:
        """Run a specific pytest test.
        
        Args:
            test_name: Name of the test (e.g., 'test-health', 'test-sql-injection')
            skip_cloudwatch: Skip CloudWatch verification for faster execution
        
        Returns:
            Dict with success status, output, and error details
        """
        if test_name not in self.ALLOWED_TESTS:
            return {
                "success": False,
                "exit_code": 1,
                "stdout": "",
                "stderr": f"Denied: '{test_name}' is not an allowed test. "
                          f"Allowed: {', '.join(sorted(self.ALLOWED_TESTS))}",
                "cached": False,
            }
        # Check cache
        cache_key = f"{test_name}:{skip_cloudwatch}"
        if cache_key in self._test_cache:
            cached_result, timestamp = self._test_cache[cache_key]
            if datetime.now().timestamp() - timestamp < self._cache_ttl:
                cached_result["cached"] = True
                return cached_result
        
        env = os.environ.copy()
        if skip_cloudwatch:
            env["SKIP_CLOUDWATCH"] = "1"
        
        returncode, stdout, stderr = self._run_command(["just", test_name], env=env)
        
        test_result = {
            "success": returncode == 0,
            "exit_code": returncode,
            "stdout": stdout[-2000:],
            "stderr": stderr[-1000:] if stderr else "",
            "cached": False,
        }
        
        # Cache successful results
        if test_result["success"]:
            self._test_cache[cache_key] = (test_result, datetime.now().timestamp())
        else:
            # Track failures for smart selection
            if test_name not in self._test_history["failures"]:
                self._test_history["failures"].append(test_name)
        
        return test_result
    
    @tool
    def run_all_tests_parallel(self, test_names: list[str] = None, skip_slow: bool = True) -> dict[str, Any]:
        """Run multiple tests in parallel for faster execution.
        
        Args:
            test_names: List of test names to run. If None, runs all fast tests.
            skip_slow: Skip slow tests (nmap, nikto, sqlmap)
        
        Returns:
            Dict with results for each test
        """
        if test_names is None:
            test_names = ["test-health", "test-headers", "test-sql-injection", "test-xss"]
        
        if skip_slow:
            test_names = [t for t in test_names if t not in self._test_history["slow_tests"]]
        
        # Use pytest-xdist for parallel execution
        returncode, stdout, stderr = self._run_command(["just", "test-parallel"])
        
        return {
            "success": returncode == 0,
            "tests_run": test_names,
            "stdout": stdout[-3000:],
            "stderr": stderr[-1000:] if stderr else "",
        }
    
    @tool
    def run_fast_tests(self) -> dict[str, Any]:
        """Run all fast tests (skips nmap, nikto, sqlmap).
        
        Returns:
            Dict with test results
        """
        returncode, stdout, stderr = self._run_command(["just", "test-fast"])
        
        return {
            "success": returncode == 0,
            "stdout": stdout[-3000:],
            "stderr": stderr[-1000:] if stderr else "",
        }
    
    @tool
    def get_test_recommendations(self) -> dict[str, Any]:
        """Get smart test recommendations based on history.
        
        Returns:
            Dict with recommended tests to run
        """
        recommendations = {
            "priority_tests": self._test_history["failures"][-5:] if self._test_history["failures"] else [],
            "fast_tests": ["test-health", "test-headers", "test-sql-injection", "test-xss"],
            "slow_tests": self._test_history["slow_tests"],
            "suggestion": "Run priority tests first if any failed recently, otherwise run all fast tests in parallel.",
        }
        return recommendations
    
    @tool
    def get_cloudformation_outputs(self, stack_name: str) -> dict[str, str]:
        """Get CloudFormation stack outputs (cached).
        
        Args:
            stack_name: Name of the stack (e.g., 'ComputeStack', 'EdgeStack')
        
        Returns:
            Dict mapping output keys to values
        """
        return self._get_cf_outputs_cached(stack_name)
    
    @lru_cache(maxsize=32)
    def _get_cf_outputs_cached(self, stack_name: str) -> dict[str, str]:
        """Cached CloudFormation outputs."""
        try:
            response = self.cf_client.describe_stacks(StackName=stack_name)
            outputs = response["Stacks"][0].get("Outputs", [])
            return {o["OutputKey"]: o["OutputValue"] for o in outputs}
        except Exception as e:
            return {"error": str(e)}
    
    @tool
    def list_log_groups(self, prefix: str = "") -> list[str]:
        """List CloudWatch log groups.
        
        Args:
            prefix: Optional prefix to filter log groups
        
        Returns:
            List of log group names
        """
        try:
            kwargs = {"limit": 50}
            if prefix:
                kwargs["logGroupNamePrefix"] = prefix
            
            response = self.logs_client.describe_log_groups(**kwargs)
            return [lg["logGroupName"] for lg in response["logGroups"]]
        except Exception as e:
            return [f"Error: {str(e)}"]
    
    @tool
    def query_logs(self, log_group: str, query: str, minutes_back: int = 10) -> list[dict]:
        """Query CloudWatch Logs Insights.
        
        Args:
            log_group: Log group name
            query: CloudWatch Logs Insights query
            minutes_back: How many minutes back to search
        
        Returns:
            List of log results
        """
        import time
        
        try:
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(minutes=minutes_back)
            
            response = self.logs_client.start_query(
                logGroupName=log_group,
                startTime=int(start_time.timestamp()),
                endTime=int(end_time.timestamp()),
                queryString=query,
            )
            
            query_id = response["queryId"]
            
            for _ in range(20):
                result = self.logs_client.get_query_results(queryId=query_id)
                if result["status"] == "Complete":
                    return result["results"][:10]  # Limit to 10 results
                time.sleep(1)
            
            return [{"error": "Query timeout"}]
        except Exception as e:
            return [{"error": str(e)}]
    
    @tool
    def list_waf_webacls(self, scope: str = "REGIONAL") -> list[dict[str, str]]:
        """List WAF Web ACLs.
        
        Args:
            scope: 'REGIONAL' or 'CLOUDFRONT'
        
        Returns:
            List of Web ACL info (name, id, arn)
        """
        try:
            response = self.wafv2_client.list_web_acls(Scope=scope)
            return [
                {
                    "name": acl["Name"],
                    "id": acl["Id"],
                    "arn": acl["ARN"],
                }
                for acl in response["WebACLs"]
            ]
        except Exception as e:
            return [{"error": str(e)}]
    
    @tool
    def get_waf_logging_config(self, resource_arn: str) -> dict[str, Any]:
        """Get WAF logging configuration.
        
        Args:
            resource_arn: ARN of the Web ACL
        
        Returns:
            Logging configuration details
        """
        try:
            response = self.wafv2_client.get_logging_configuration(ResourceArn=resource_arn)
            config = response["LoggingConfiguration"]
            return {
                "log_destinations": config["LogDestinationConfigs"],
                "log_type": config.get("LogType", "WAF_LOGS"),
            }
        except Exception as e:
            return {"error": str(e)}
    
    @tool
    def read_test_file(self, file_path: str = "test_pentest.py") -> str:
        """Read a file from the testing directory.
        
        Args:
            file_path: Path to file relative to testing directory (must stay within testing/)
        
        Returns:
            File content
        """
        try:
            full_path = (self.testing_dir / file_path).resolve()
            if not str(full_path).startswith(str(self.testing_dir)):
                return "Error: Access denied. Can only read files inside testing/ directory."
            return full_path.read_text()
        except Exception as e:
            return f"Error: {str(e)}"
    
    @tool
    def fix_test_code(self, file_path: str, old_code: str, new_code: str, reason: str) -> dict[str, Any]:
        """Fix test code by replacing old code with new code.
        
        Args:
            file_path: Path to file relative to testing directory (must stay within testing/)
            old_code: Code to replace (must match exactly)
            new_code: New code to insert
            reason: Explanation of why this fix is needed
        
        Returns:
            Success status and message
        """
        try:
            full_path = (self.testing_dir / file_path).resolve()
            if not str(full_path).startswith(str(self.testing_dir)):
                return {
                    "success": False,
                    "message": "Access denied. Can only modify files inside testing/ directory.",
                }
            
            content = full_path.read_text()
            
            if old_code not in content:
                return {
                    "success": False,
                    "message": "Old code not found in file. Make sure it matches exactly.",
                }
            
            new_content = content.replace(old_code, new_code)
            full_path.write_text(new_content)
            
            return {
                "success": True,
                "message": f"Fixed: {reason}",
            }
        except Exception as e:
            return {"success": False, "message": f"Error: {str(e)}"}
    
    async def run_async(self):
        """Start the interactive agent."""
        print("ðŸ”’ Penetration Testing Agent Ready!")
        print("I can help you run security tests, debug failures, and fix issues.\n")
        
        while True:
            try:
                user_input = input("\nYou: ").strip()
                if not user_input or user_input.lower() in ["exit", "quit"]:
                    print("Goodbye!")
                    break
                
                # Stream response
                print("\nAgent: ", end="", flush=True)
                async for chunk in self.agent.stream_async(user_input):
                    if hasattr(chunk, 'content'):
                        print(chunk.content, end="", flush=True)
                print()  # New line after response
                
            except KeyboardInterrupt:
                print("\nGoodbye!")
                break
            except Exception as e:
                print(f"\nError: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    def run(self):
        """Start the interactive agent (sync wrapper)."""
        import asyncio
        asyncio.run(self.run_async())


if __name__ == "__main__":
    agent = PentestAgent()
    agent.run()
